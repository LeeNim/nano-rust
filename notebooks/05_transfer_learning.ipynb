{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 05 — Transfer Learning: Frozen Backbone + Trainable Head\n",
                "\n",
                "**Goal**: Demonstrate the hybrid memory pattern for MCU deployment.\n",
                "\n",
                "**Architecture**:\n",
                "```\n",
                "FLASH (read-only, 0 bytes RAM):           RAM (trainable):\n",
                "┌───────────────────────┐                 ┌─────────────────┐\n",
                "│ FrozenDense(784→128)  │ ──→ ReLU ──→    │ TrainableDense  │\n",
                "│ (100KB weights)       │                 │ (128→10, 1.3KB) │\n",
                "└───────────────────────┘                 └─────────────────┘\n",
                "```\n",
                "\n",
                "**Why this matters for MCU**:\n",
                "- **Backbone in Flash**: Pre-trained feature extractor. Loaded at boot, never modified.\n",
                "  Cost: N bytes Flash, **0 bytes RAM**.\n",
                "- **Head in RAM**: Small classifier layer. Can be retrained on-device with new data.\n",
                "  Cost: `in × out + out` bytes RAM.\n",
                "\n",
                "This lets you deploy a large pre-trained model but adapt it to local conditions\n",
                "(e.g., a factory sensor that needs to learn the specific machine's vibration pattern).\n",
                "\n",
                "**Prerequisites**: `pip install nano-rust-py numpy torch`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "from nano_rust_py.utils import quantize_to_i8, quantize_weights, calibrate_model\n",
                "import nano_rust_py\n",
                "\n",
                "print('✅ All imports OK')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Memory Budget Analysis\n",
                "\n",
                "Before building, let's plan the memory:\n",
                "\n",
                "| Component | Flash | RAM |\n",
                "|-----------|-------|-----|\n",
                "| Backbone weights (784×128) | 100,352 B | 0 B |\n",
                "| Backbone bias (128) | 128 B | 0 B |\n",
                "| Head weights (128×10) | 0 B | 1,280 B |\n",
                "| Head bias (10) | 0 B | 10 B |\n",
                "| Arena (intermediates) | 0 B | ~512 B |\n",
                "| **Total** | **~100KB** | **~1.8KB** |\n",
                "| **ESP32 Available** | **4MB** | **520KB** |\n",
                "\n",
                "We use less than 3% of Flash and less than 1% of RAM. Plenty of room!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Build the PyTorch model (for weight extraction) ---\n",
                "torch.manual_seed(42)\n",
                "backbone = nn.Linear(784, 128)\n",
                "head = nn.Linear(128, 10)\n",
                "full_model = nn.Sequential(backbone, nn.ReLU(), head)\n",
                "full_model.eval()\n",
                "\n",
                "backbone_size = backbone.weight.numel() + backbone.bias.numel()\n",
                "head_size = head.weight.numel() + head.bias.numel()\n",
                "print(f'Backbone: {backbone_size:,} params → {backbone_size:,} bytes in Flash')\n",
                "print(f'Head:     {head_size:,} params → {head_size:,} bytes in RAM')\n",
                "print(f'Ratio:    {backbone_size/head_size:.0f}:1 (backbone is {backbone_size/head_size:.0f}× larger)')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Quantize Backbone, Build Hybrid Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quantize backbone weights\n",
                "q_weights = quantize_weights(full_model)\n",
                "\n",
                "# Calibrate\n",
                "test_input = torch.randn(1, 784)\n",
                "q_input, input_scale = quantize_to_i8(test_input.numpy().flatten())\n",
                "requant = calibrate_model(full_model, test_input, q_weights, input_scale)\n",
                "\n",
                "# Build NANO model with hybrid architecture\n",
                "nano = nano_rust_py.PySequentialModel([784], arena_size=4096)\n",
                "\n",
                "# FROZEN backbone (weights in Flash)\n",
                "m, s, b = requant['0']\n",
                "nano.add_dense_with_requant(\n",
                "    q_weights['0']['weights'].flatten().tolist(), b, m, s\n",
                ")\n",
                "nano.add_relu()\n",
                "\n",
                "# TRAINABLE head (weights in RAM)\n",
                "# In real deployment: head can be retrained on-device\n",
                "nano.add_trainable_dense(128, 10)\n",
                "\n",
                "print('✅ Hybrid model built')\n",
                "print('   Backbone: FrozenDense (Flash, 0 RAM)')\n",
                "print('   Head: TrainableDense (RAM, retrainable)')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Run Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "output = nano.forward(q_input.tolist())\n",
                "predicted = nano.predict(q_input.tolist())\n",
                "\n",
                "print(f'Raw output (i8): {output}')\n",
                "print(f'Predicted class: {predicted}')\n",
                "print(f'\\nNote: Since TrainableDense initializes with small random weights,')\n",
                "print(f'the output is essentially random. In real deployment, you would')\n",
                "print(f'retrain the head on-device with local data.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary: The Hybrid Memory Strategy\n",
                "\n",
                "```\n",
                "┌──────────────────────────────────────────────────────┐\n",
                "│  PRE-DEPLOYMENT (PC/GPU)                             │\n",
                "│  1. Train backbone on large dataset                  │\n",
                "│  2. Quantize backbone → i8                           │\n",
                "│  3. Export to .rs (static arrays in Flash)            │\n",
                "├──────────────────────────────────────────────────────┤\n",
                "│  ON-DEVICE (MCU)                                     │\n",
                "│  4. Load backbone from Flash (free!)                 │\n",
                "│  5. Initialize small head in RAM                     │\n",
                "│  6. Collect local samples                            │\n",
                "│  7. Retrain head (gradient descent in i8)            │\n",
                "│  8. Run inference: backbone(x) → head(features)      │\n",
                "└──────────────────────────────────────────────────────┘\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

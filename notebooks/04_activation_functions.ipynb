{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 04 — Activation Functions Comparison (Scale-Aware)\n",
                "\n",
                "Same MLP with ReLU / Sigmoid / Tanh — uses **scaled** LUT for Sigmoid/Tanh.\n",
                "\n",
                "Key: Sigmoid/Tanh LUTs assume fixed input scale (x/16, x/32). After requantization,\n",
                "actual scale is S_y. Fix: rescale `index = (i8_val * mult) >> shift` before LUT."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from _setup import setup_all, PROJECT_ROOT\n",
                "setup_all()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from nano_rust_utils import quantize_to_i8, quantize_weights, calibrate_model\n",
                "import nano_rust_py\n",
                "\n",
                "torch.manual_seed(42)\n",
                "base = nn.Sequential(nn.Linear(16, 32), nn.ReLU(), nn.Linear(32, 10))\n",
                "base.eval()\n",
                "w0, b0 = base[0].weight.data.clone(), base[0].bias.data.clone()\n",
                "w2, b2 = base[2].weight.data.clone(), base[2].bias.data.clone()\n",
                "\n",
                "torch.manual_seed(123)\n",
                "input_tensor = torch.randn(1, 16)\n",
                "q_input, input_scale = quantize_to_i8(input_tensor.numpy().flatten())\n",
                "\n",
                "activations = {'ReLU': nn.ReLU(), 'Sigmoid': nn.Sigmoid(), 'Tanh': nn.Tanh()}\n",
                "results = {}\n",
                "\n",
                "for act_name, act_module in activations.items():\n",
                "    model = nn.Sequential(nn.Linear(16, 32), act_module, nn.Linear(32, 10))\n",
                "    model[0].weight.data = w0.clone()\n",
                "    model[0].bias.data = b0.clone()\n",
                "    model[2].weight.data = w2.clone()\n",
                "    model[2].bias.data = b2.clone()\n",
                "    model.eval()\n",
                "\n",
                "    q_weights = quantize_weights(model)\n",
                "    requant = calibrate_model(model, input_tensor, q_weights, input_scale)\n",
                "\n",
                "    with torch.no_grad():\n",
                "        pytorch_out = model(input_tensor).numpy().flatten()\n",
                "\n",
                "    nano = nano_rust_py.PySequentialModel(input_shape=[16], arena_size=8192)\n",
                "    \n",
                "    # Layer 0: Dense(16->32)\n",
                "    m0r, s0r, b0r = requant['0']\n",
                "    nano.add_dense_with_requant(\n",
                "        q_weights['0']['weights'].flatten().tolist(), b0r, m0r, s0r)\n",
                "    \n",
                "    # Layer 1: Activation (scale-aware for Sigmoid/Tanh)\n",
                "    if act_name == 'ReLU':\n",
                "        nano.add_relu()\n",
                "    elif act_name == 'Sigmoid':\n",
                "        _, sm, ss = requant['1']  # ('sigmoid', mult, shift)\n",
                "        nano.add_sigmoid_scaled(sm, ss)\n",
                "    elif act_name == 'Tanh':\n",
                "        _, sm, ss = requant['1']  # ('tanh', mult, shift)\n",
                "        nano.add_tanh_scaled(sm, ss)\n",
                "    \n",
                "    # Layer 2: Dense(32->10)\n",
                "    m2r, s2r, b2r = requant['2']\n",
                "    nano.add_dense_with_requant(\n",
                "        q_weights['2']['weights'].flatten().tolist(), b2r, m2r, s2r)\n",
                "    nano_out = nano.forward(q_input.tolist())\n",
                "\n",
                "    q_pytorch, _ = quantize_to_i8(pytorch_out)\n",
                "    nano_arr = np.array(nano_out, dtype=np.int8)\n",
                "    diff = np.abs(q_pytorch.astype(np.int32) - nano_arr.astype(np.int32))\n",
                "    results[act_name] = {\n",
                "        'max_diff': int(np.max(diff)), 'mean_diff': float(np.mean(diff)),\n",
                "        'pytorch_class': int(np.argmax(q_pytorch)), 'nano_class': int(np.argmax(nano_arr)),\n",
                "    }\n",
                "\n",
                "print('=' * 70)\n",
                "print(f'{\"Activation\":<12} {\"Max Diff\":<12} {\"Mean Diff\":<12} {\"Class Match\":<12} {\"Result\"}')\n",
                "print('-' * 70)\n",
                "for name, r in results.items():\n",
                "    tol = 20\n",
                "    match = 'Yes' if r['pytorch_class'] == r['nano_class'] else 'No'\n",
                "    passed = '✅ PASS' if r['max_diff'] <= tol else '❌ FAIL'\n",
                "    print(f'{name:<12} {r[\"max_diff\"]:<12} {r[\"mean_diff\"]:<12.2f} {match:<12} {passed} (tol={tol})')\n",
                "print('=' * 70)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
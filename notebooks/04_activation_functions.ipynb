{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 04 — Activation Functions Comparison\n",
                "\n",
                "**Goal**: Compare all activation functions available in NANO-RUST and understand\n",
                "when to use each one.\n",
                "\n",
                "**Activations tested**:\n",
                "| Activation | Method | MCU Cost | Best For |\n",
                "|------------|--------|----------|----------|\n",
                "| ReLU | `add_relu()` | Fastest (branchless) | Hidden layers |\n",
                "| Sigmoid (fixed) | `add_sigmoid()` | LUT lookup | General use |\n",
                "| Sigmoid (scaled) | `add_sigmoid_scaled(m, s)` | LUT + rescale | After calibration |\n",
                "| Tanh (fixed) | `add_tanh()` | LUT lookup | Centered output |\n",
                "| Tanh (scaled) | `add_tanh_scaled(m, s)` | LUT + rescale | After calibration |\n",
                "| Softmax | `add_softmax()` | Approximation | Output layer (probabilities) |\n",
                "\n",
                "**Key insight**: Fixed sigmoid/tanh use a hardcoded input divisor (16/32),\n",
                "which may not match your actual input scale. Scaled variants let you provide\n",
                "the exact `(mult, shift)` from calibration for much better accuracy.\n",
                "\n",
                "**Prerequisites**: `pip install nano-rust-py numpy torch`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "sys.path.insert(0, str(Path.cwd().parent / 'scripts'))\n",
                "from nano_rust_utils import (\n",
                "    quantize_to_i8, quantize_weights,\n",
                "    calibrate_model, compute_activation_scale_params\n",
                ")\n",
                "import nano_rust_py\n",
                "\n",
                "print('✅ All imports OK')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: ReLU — The Default Choice\n",
                "\n",
                "`ReLU(x) = max(0, x)` — zero cost on MCU (just a comparison).\n",
                "Use this for all hidden layers unless you have a specific reason not to."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ReLU test: negative values become 0, positive values pass through\n",
                "torch.manual_seed(42)\n",
                "model_relu = nn.Sequential(nn.Linear(8, 8), nn.ReLU())\n",
                "model_relu.eval()\n",
                "\n",
                "q_weights = quantize_weights(model_relu)\n",
                "test_input = torch.randn(1, 8)\n",
                "q_input, scale = quantize_to_i8(test_input.numpy().flatten())\n",
                "requant = calibrate_model(model_relu, test_input, q_weights, scale)\n",
                "\n",
                "nano = nano_rust_py.PySequentialModel([8], 1024)\n",
                "m, s, b = requant['0']\n",
                "nano.add_dense_with_requant(q_weights['0']['weights'].flatten().tolist(), b, m, s)\n",
                "nano.add_relu()\n",
                "\n",
                "result = nano.forward(q_input.tolist())\n",
                "print(f'ReLU output: {result}')\n",
                "print(f'All non-negative: {all(v >= 0 for v in result)} ✅')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Sigmoid — For Binary Classification\n",
                "\n",
                "`Sigmoid(x) = 1 / (1 + e^(-x))` → output in [0, 1].\n",
                "\n",
                "In i8: maps to approximately [0, 127] (positive half of i8 range).\n",
                "\n",
                "Two variants:\n",
                "- **Fixed** (`add_sigmoid()`): assumes input_i8 / 16 as float input\n",
                "- **Scaled** (`add_sigmoid_scaled(m, s)`): uses calibrated scale"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sigmoid: fixed vs scaled comparison\n",
                "torch.manual_seed(42)\n",
                "model_sig = nn.Sequential(nn.Linear(8, 8), nn.Sigmoid())\n",
                "model_sig.eval()\n",
                "\n",
                "q_weights = quantize_weights(model_sig)\n",
                "test_input = torch.randn(1, 8)\n",
                "q_input, scale = quantize_to_i8(test_input.numpy().flatten())\n",
                "requant = calibrate_model(model_sig, test_input, q_weights, scale)\n",
                "\n",
                "# Fixed sigmoid (no calibration)\n",
                "nano_fixed = nano_rust_py.PySequentialModel([8], 1024)\n",
                "m, s, b = requant['0']\n",
                "nano_fixed.add_dense_with_requant(q_weights['0']['weights'].flatten().tolist(), b, m, s)\n",
                "nano_fixed.add_sigmoid()\n",
                "fixed_out = nano_fixed.forward(q_input.tolist())\n",
                "\n",
                "# Scaled sigmoid (with calibration)\n",
                "# Compute scaled params from the intermediate activation scale\n",
                "sig_m, sig_s = compute_activation_scale_params(scale * q_weights['0']['weight_scale'], 16.0)\n",
                "nano_scaled = nano_rust_py.PySequentialModel([8], 1024)\n",
                "nano_scaled.add_dense_with_requant(q_weights['0']['weights'].flatten().tolist(), b, m, s)\n",
                "nano_scaled.add_sigmoid_scaled(sig_m, sig_s)\n",
                "scaled_out = nano_scaled.forward(q_input.tolist())\n",
                "\n",
                "# PyTorch reference\n",
                "with torch.no_grad():\n",
                "    ref = model_sig(test_input).numpy().flatten()\n",
                "q_ref, _ = quantize_to_i8(ref)\n",
                "\n",
                "print(f'PyTorch (i8):     {q_ref.tolist()}')\n",
                "print(f'Fixed sigmoid:    {fixed_out}')\n",
                "print(f'Scaled sigmoid:   {scaled_out}')\n",
                "print(f'\\nFixed diff:  {np.max(np.abs(q_ref - np.array(fixed_out, np.int8)))}')\n",
                "print(f'Scaled diff: {np.max(np.abs(q_ref - np.array(scaled_out, np.int8)))}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Tanh — Centered Output\n",
                "\n",
                "`Tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))` → output in [-1, 1].\n",
                "\n",
                "In i8: maps to [-127, 127]. Useful when you need centered (zero-mean) outputs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nano_tanh = nano_rust_py.PySequentialModel([8], 1024)\n",
                "nano_tanh.add_dense_with_requant(\n",
                "    q_weights['0']['weights'].flatten().tolist(), b, m, s\n",
                ")\n",
                "nano_tanh.add_tanh()\n",
                "tanh_out = nano_tanh.forward(q_input.tolist())\n",
                "print(f'Tanh output: {tanh_out}')\n",
                "print(f'Range: [{min(tanh_out)}, {max(tanh_out)}]')\n",
                "print('Centered around 0 ✅' if abs(np.mean(tanh_out)) < 64 else 'Not well centered')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Softmax — Multi-class Output\n",
                "\n",
                "Pseudo-softmax approximation for i8. Output values represent relative class\n",
                "scores — higher = more likely. Not true probabilities but sufficient for `argmax`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nano_sm = nano_rust_py.PySequentialModel([8], 1024)\n",
                "nano_sm.add_dense_with_requant(\n",
                "    q_weights['0']['weights'].flatten().tolist(), b, m, s\n",
                ")\n",
                "nano_sm.add_softmax()\n",
                "sm_out = nano_sm.forward(q_input.tolist())\n",
                "print(f'Softmax output: {sm_out}')\n",
                "print(f'Predicted class: {np.argmax(sm_out)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "| Activation | Speed | Accuracy | Use Case |\n",
                "|------------|-------|----------|----------|\n",
                "| ReLU | ⚡⚡⚡ | Best | Default for hidden layers |\n",
                "| Sigmoid (fixed) | ⚡⚡ | OK | Quick prototyping |\n",
                "| Sigmoid (scaled) | ⚡⚡ | Best | After calibration |\n",
                "| Tanh | ⚡⚡ | OK | Centered features needed |\n",
                "| Softmax | ⚡ | N/A | Output layer only |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
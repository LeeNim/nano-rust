{
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üè≠ 08 ‚Äî Industrial Sensor Anomaly Detection\n",
                "\n",
                "**MLP on IMU sensor data ‚Äî classify machine health from vibration patterns**\n",
                "\n",
                "| Property | Value |\n",
                "|----------|-------|\n",
                "| **Task** | Machine health classification from IMU data |\n",
                "| **Input** | 6 features: accel_x/y/z + gyro_x/y/z |\n",
                "| **Classes** | Normal, Bearing Fault, Misalignment, Imbalance |\n",
                "| **Architecture** | Dense(6‚Üí32) ‚Üí ReLU ‚Üí Dense(32‚Üí16) ‚Üí ReLU ‚Üí Dense(16‚Üí4) |\n",
                "| **MCU Memory** | ~820 bytes Flash + 4KB Arena |\n",
                "\n",
                "> This MLP is tiny enough to fit on **any microcontroller** ‚Äî even ATmega328 (2KB RAM)!\n",
                "\n",
                "### Why Global Scale?\n",
                "Sensor data has variable amplitude per sample. Using per-sample quantization causes\n",
                "calibration mismatch. In real MCU deployment, you'd set a fixed input range from\n",
                "sensor datasheet specs (e.g., ¬±16g for accelerometer).\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import time\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import TensorDataset, DataLoader\n",
                "from nano_rust_py.utils import quantize_to_i8, quantize_weights, calibrate_model\n",
                "import nano_rust_py\n",
                "\n",
                "CLASSES = ['Normal', 'Bearing Fault', 'Misalignment', 'Imbalance']\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Device: {device}')\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Generate Synthetic IMU Sensor Data\n",
                "\n",
                "Each class has a distinct vibration signature:\n",
                "\n",
                "| Class | Pattern |\n",
                "|-------|--------|\n",
                "| Normal | Low amplitude noise |\n",
                "| Bearing Fault | High-frequency spikes in accelerometer |\n",
                "| Misalignment | Correlated accel/gyro signals |\n",
                "| Imbalance | Periodic oscillation in one axis |\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "np.random.seed(42)\n",
                "N_PER_CLASS = 500\n",
                "N_FEATURES = 6\n",
                "\n",
                "def generate_sensor_data(n, class_id):\n",
                "    base = np.random.randn(n, N_FEATURES) * 0.3\n",
                "    if class_id == 0:    base *= 0.5                               # Normal\n",
                "    elif class_id == 1:  base[:, :3] += np.random.choice([-1, 1], (n, 3)) * np.random.exponential(0.8, (n, 3))  # Bearing\n",
                "    elif class_id == 2:  base[:, 3:] = base[:, :3] * 0.7 + np.random.randn(n, 3) * 0.2  # Misalign\n",
                "    elif class_id == 3:                                             # Imbalance\n",
                "        base[:, 0] += np.sin(np.linspace(0, 4*np.pi, n)) * 1.2\n",
                "        base[:, 3] += np.cos(np.linspace(0, 4*np.pi, n)) * 0.8\n",
                "    return base\n",
                "\n",
                "X_all, y_all = [], []\n",
                "for c in range(4):\n",
                "    X_all.append(generate_sensor_data(N_PER_CLASS, c))\n",
                "    y_all.extend([c] * N_PER_CLASS)\n",
                "\n",
                "X = np.vstack(X_all).astype(np.float32)\n",
                "y = np.array(y_all, dtype=np.int64)\n",
                "idx = np.random.permutation(len(X))\n",
                "X, y = X[idx], y[idx]\n",
                "split = int(0.8 * len(X))\n",
                "X_train, X_test = X[:split], X[split:]\n",
                "y_train, y_test = y[:split], y[split:]\n",
                "\n",
                "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
                "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, pin_memory=True)\n",
                "print(f'Train: {len(X_train)} | Test: {len(X_test)} | Features: {N_FEATURES}')\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Train MLP"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "model = nn.Sequential(\n",
                "    nn.Linear(6, 32),  nn.ReLU(),\n",
                "    nn.Linear(32, 16), nn.ReLU(),\n",
                "    nn.Linear(16, 4),\n",
                ").to(device)\n",
                "\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "EPOCHS = 20\n",
                "\n",
                "t0 = time.time()\n",
                "for epoch in range(EPOCHS):\n",
                "    model.train()\n",
                "    correct, total = 0, 0\n",
                "    for data, target in train_loader:\n",
                "        data, target = data.to(device), target.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        out = model(data)\n",
                "        criterion(out, target).backward()\n",
                "        optimizer.step()\n",
                "        correct += out.argmax(1).eq(target).sum().item()\n",
                "        total += target.size(0)\n",
                "    if (epoch + 1) % 5 == 0:\n",
                "        print(f'  Epoch {epoch+1}/{EPOCHS} ‚Äî Acc: {100.*correct/total:.1f}%')\n",
                "train_time = time.time() - t0\n",
                "print(f'Training: {train_time:.1f}s')\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Quantize with Global Scale\n\nWe use a fixed global scale computed from the entire dataset to ensure consistent quantization."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "model_cpu = model.cpu().eval()\n",
                "q_weights = quantize_weights(model_cpu)\n",
                "\n",
                "# Global scale from entire dataset\n",
                "global_max = float(np.max(np.abs(np.vstack([X_train, X_test]))))\n",
                "global_scale = global_max / 127.0\n",
                "print(f'Global input scale: {global_scale:.6f} (max_abs={global_max:.4f})')\n",
                "\n",
                "def quantize_global(data):\n",
                "    return np.clip(np.round(data / global_scale), -128, 127).astype(np.int8)\n",
                "\n",
                "cal_input = torch.from_numpy(X_test[:1])\n",
                "requant = calibrate_model(model_cpu, cal_input, q_weights, global_scale)\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: NANO-RUST Inference & Comparison"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def build_nano():\n",
                "    nano = nano_rust_py.PySequentialModel(input_shape=[6], arena_size=4096)\n",
                "    m, s, bc = requant['0']\n",
                "    nano.add_dense_with_requant(q_weights['0']['weights'].flatten().tolist(), bc, m, s)\n",
                "    nano.add_relu()\n",
                "    m, s, bc = requant['2']\n",
                "    nano.add_dense_with_requant(q_weights['2']['weights'].flatten().tolist(), bc, m, s)\n",
                "    nano.add_relu()\n",
                "    m, s, bc = requant['4']\n",
                "    nano.add_dense_with_requant(q_weights['4']['weights'].flatten().tolist(), bc, m, s)\n",
                "    return nano\n",
                "\n",
                "correct_pt, correct_nano, match_count = 0, 0, 0\n",
                "max_diffs = []\n",
                "\n",
                "t0 = time.time()\n",
                "for i in range(len(X_test)):\n",
                "    x_f = torch.from_numpy(X_test[i:i+1])\n",
                "    label = int(y_test[i])\n",
                "    q_x = quantize_global(X_test[i])\n",
                "    with torch.no_grad():\n",
                "        pt_out = model_cpu(x_f).numpy().flatten()\n",
                "    pt_cls = int(np.argmax(pt_out))\n",
                "    nano_out = build_nano().forward(q_x.tolist())\n",
                "    nano_cls = int(np.argmax(nano_out))\n",
                "    q_pt, _ = quantize_to_i8(pt_out)\n",
                "    diff = np.abs(q_pt.astype(np.int32) - np.array(nano_out, dtype=np.int8).astype(np.int32))\n",
                "    max_diffs.append(int(np.max(diff)))\n",
                "    if pt_cls == label: correct_pt += 1\n",
                "    if nano_cls == label: correct_nano += 1\n",
                "    if nano_cls == pt_cls: match_count += 1\n",
                "infer_time = time.time() - t0\n",
                "N = len(X_test)\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Results"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "total_w = sum(q['weights'].nbytes for q in q_weights.values())\n",
                "print('=' * 60)\n",
                "print('       SENSOR ANOMALY DETECTION RESULTS')\n",
                "print('=' * 60)\n",
                "print(f'PyTorch Accuracy:     {100.*correct_pt/N:.1f}%')\n",
                "print(f'NANO-RUST Accuracy:   {100.*correct_nano/N:.1f}%')\n",
                "print(f'Classification Match: {100.*match_count/N:.1f}%')\n",
                "print(f'Max Diff (median):    {int(np.median(max_diffs))}')\n",
                "print(f'Memory: {total_w} bytes weights + 4KB arena')\n",
                "print(f'Fits ATmega328 (2KB RAM)? {\"YES\" if total_w < 2048 else \"NO\"}')\n",
                "print('=' * 60)\n",
                "print(f'{\"‚úÖ PASS\" if 100.*match_count/N > 85 else \"‚ùå FAIL\"}: {100.*match_count/N:.1f}% agreement')\n"
            ],
            "outputs": [],
            "execution_count": null
        }
    ]
}

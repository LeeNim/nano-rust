{
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üì∏ 06 ‚Äî MNIST Digit Classification\n",
                "\n",
                "**Train CNN on MNIST ‚Üí Quantize to i8 ‚Üí Verify on NANO-RUST engine**\n",
                "\n",
                "| Property | Value |\n",
                "|----------|-------|\n",
                "| **Task** | Handwritten digit recognition (0‚Äì9) |\n",
                "| **Dataset** | MNIST ‚Äî 60k train / 10k test (28√ó28 grayscale) |\n",
                "| **Architecture** | Conv2D(1‚Üí8) ‚Üí ReLU ‚Üí Pool ‚Üí Conv2D(8‚Üí16) ‚Üí ReLU ‚Üí Pool ‚Üí Flatten ‚Üí Dense(784‚Üí10) |\n",
                "| **Expected Accuracy** | ~97% (PyTorch), ~95‚Äì97% (NANO i8) |\n",
                "| **MCU Memory** | ~13KB Flash + 32KB Arena |\n",
                "\n",
                "> **Pipeline**: Train (GPU) ‚Üí Quantize (float32‚Üíint8) ‚Üí Calibrate ‚Üí Build NANO model ‚Üí Verify\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 0: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# !pip install nano-rust-py[train] torchvision\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Setup & GPU Detection\n\nWe use CUDA if available for fast training, then move to CPU for quantization."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import time\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torchvision import datasets, transforms\n",
                "from torch.utils.data import DataLoader\n",
                "from nano_rust_py.utils import quantize_to_i8, quantize_weights, calibrate_model\n",
                "import nano_rust_py\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Device: {device}')\n",
                "if device.type == 'cuda':\n",
                "    print(f'  GPU: {torch.cuda.get_device_name(0)}')\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Load MNIST Dataset\n",
                "\n",
                "Each image is 28√ó28 grayscale, normalized with dataset-specific mean/std.\n",
                "\n",
                "| Param | Value | Why |\n",
                "|-------|-------|-----|\n",
                "| Normalize mean | 0.1307 | Centers pixel distribution |\n",
                "| Normalize std | 0.3081 | Scales to unit variance |\n",
                "| Batch size | 256 | Good GPU utilization |\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "transform = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize((0.1307,), (0.3081,))\n",
                "])\n",
                "\n",
                "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
                "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, pin_memory=True, num_workers=0)\n",
                "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False, pin_memory=True, num_workers=0)\n",
                "\n",
                "print(f'Train: {len(train_dataset):,} images')\n",
                "print(f'Test:  {len(test_dataset):,} images')\n",
                "print(f'Image shape: {train_dataset[0][0].shape}  (C, H, W)')\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Define & Train CNN\n",
                "\n",
                "Architecture designed for MCU deployment ‚Äî small kernels, few channels:\n",
                "\n",
                "```\n",
                "Input [1,28,28] ‚Üí Conv(1‚Üí8, 3√ó3) ‚Üí ReLU ‚Üí Pool(2) ‚Üí [8,14,14]\n",
                "               ‚Üí Conv(8‚Üí16, 3√ó3) ‚Üí ReLU ‚Üí Pool(2) ‚Üí [16,7,7]\n",
                "               ‚Üí Flatten ‚Üí [784] ‚Üí Dense ‚Üí [10]\n",
                "```\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "model = nn.Sequential(\n",
                "    nn.Conv2d(1, 8, 3, stride=1, padding=1),   # [1,28,28] ‚Üí [8,28,28]\n",
                "    nn.ReLU(),\n",
                "    nn.MaxPool2d(2, 2),                          # ‚Üí [8,14,14]\n",
                "    nn.Conv2d(8, 16, 3, stride=1, padding=1),   # ‚Üí [16,14,14]\n",
                "    nn.ReLU(),\n",
                "    nn.MaxPool2d(2, 2),                          # ‚Üí [16,7,7]\n",
                "    nn.Flatten(),                                # ‚Üí [784]\n",
                "    nn.Linear(16 * 7 * 7, 10),                  # ‚Üí [10]\n",
                ").to(device)\n",
                "\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "print(f'Parameters: {total_params:,}')\n",
                "print(f'Float32: {total_params * 4:,} bytes ‚Üí Int8: {total_params:,} bytes (4x smaller!)')\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "EPOCHS = 3\n",
                "\n",
                "t0 = time.time()\n",
                "for epoch in range(EPOCHS):\n",
                "    model.train()\n",
                "    correct, total = 0, 0\n",
                "    for data, target in train_loader:\n",
                "        data, target = data.to(device), target.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        output = model(data)\n",
                "        loss = criterion(output, target)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        correct += output.argmax(1).eq(target).sum().item()\n",
                "        total += target.size(0)\n",
                "    print(f'  Epoch {epoch+1}/{EPOCHS} ‚Äî Acc: {100.*correct/total:.1f}%')\n",
                "\n",
                "train_time = time.time() - t0\n",
                "print(f'\\nTraining complete in {train_time:.1f}s')\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Evaluate PyTorch Baseline\n\nThis is our **float32 baseline**. NANO should be within 2-3%."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "model.eval()\n",
                "correct_pt, total_pt = 0, 0\n",
                "with torch.no_grad():\n",
                "    for data, target in test_loader:\n",
                "        data, target = data.to(device), target.to(device)\n",
                "        correct_pt += model(data).argmax(1).eq(target).sum().item()\n",
                "        total_pt += target.size(0)\n",
                "\n",
                "pt_accuracy = 100. * correct_pt / total_pt\n",
                "print(f'‚úÖ PyTorch Test Accuracy: {pt_accuracy:.2f}%')\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Quantize & Calibrate\n",
                "\n",
                "1. **`quantize_weights()`** ‚Äî float32 ‚Üí int8 (4√ó compression)\n",
                "2. **`calibrate_model()`** ‚Äî compute `(requant_m, requant_shift)` per layer\n",
                "\n",
                "> Without calibration: ~85% accuracy. With calibration: **95‚Äì99%**\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "model_cpu = model.cpu().eval()\n",
                "\n",
                "# Quantize weights\n",
                "q_weights = quantize_weights(model_cpu)\n",
                "for name, info in q_weights.items():\n",
                "    w = info['weights']\n",
                "    print(f'  Layer {name} ({info[\"type\"]}): {w.shape}, {w.nbytes} bytes, scale={info[\"weight_scale\"]:.6f}')\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Calibrate requantization parameters\n",
                "cal_image = test_dataset[0][0].unsqueeze(0)\n",
                "q_cal, cal_scale = quantize_to_i8(cal_image.numpy().flatten())\n",
                "requant = calibrate_model(model_cpu, cal_image, q_weights, cal_scale)\n",
                "\n",
                "for name, params in requant.items():\n",
                "    if isinstance(params, tuple) and len(params) == 3:\n",
                "        m, s, _ = params\n",
                "        print(f'  Layer {name}: requant_m={m}, requant_shift={s}')\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Build NANO Model & Compare\n",
                "\n",
                "Build the same architecture in NANO-RUST with i8 weights + calibrated requantization,\n",
                "then compare predictions on 1000 test images.\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def build_nano_model():\n",
                "    nano = nano_rust_py.PySequentialModel(input_shape=[1, 28, 28], arena_size=131072)\n",
                "    # Conv2d(1‚Üí8) + ReLU + Pool\n",
                "    m, s, bc = requant['0']\n",
                "    nano.add_conv2d_with_requant(\n",
                "        q_weights['0']['weights'].flatten().tolist(), bc, 1, 8, 3, 3, 1, 1, m, s)\n",
                "    nano.add_relu()\n",
                "    nano.add_max_pool2d(2, 2, 2)\n",
                "    # Conv2d(8‚Üí16) + ReLU + Pool\n",
                "    m, s, bc = requant['3']\n",
                "    nano.add_conv2d_with_requant(\n",
                "        q_weights['3']['weights'].flatten().tolist(), bc, 8, 16, 3, 3, 1, 1, m, s)\n",
                "    nano.add_relu()\n",
                "    nano.add_max_pool2d(2, 2, 2)\n",
                "    # Flatten + Dense(784‚Üí10)\n",
                "    nano.add_flatten()\n",
                "    m, s, bc = requant['7']\n",
                "    nano.add_dense_with_requant(\n",
                "        q_weights['7']['weights'].flatten().tolist(), bc, m, s)\n",
                "    return nano\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "N_TEST = min(1000, len(test_dataset))\n",
                "correct_nano, match_count = 0, 0\n",
                "max_diffs = []\n",
                "\n",
                "t0 = time.time()\n",
                "for i in range(N_TEST):\n",
                "    image, label = test_dataset[i]\n",
                "    q_image, _ = quantize_to_i8(image.numpy().flatten())\n",
                "    nano_out = build_nano_model().forward(q_image.tolist())\n",
                "    nano_cls = int(np.argmax(nano_out))\n",
                "\n",
                "    with torch.no_grad():\n",
                "        pt_out = model_cpu(image.unsqueeze(0)).numpy().flatten()\n",
                "    pt_cls = int(np.argmax(pt_out))\n",
                "\n",
                "    q_pt, _ = quantize_to_i8(pt_out)\n",
                "    diff = np.abs(q_pt.astype(np.int32) - np.array(nano_out, dtype=np.int8).astype(np.int32))\n",
                "    max_diffs.append(int(np.max(diff)))\n",
                "    if nano_cls == label: correct_nano += 1\n",
                "    if nano_cls == pt_cls: match_count += 1\n",
                "    if (i+1) % 250 == 0: print(f'  {i+1}/{N_TEST}...')\n",
                "\n",
                "infer_time = time.time() - t0\n",
                "print(f'Done in {infer_time:.1f}s')\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Results"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "nano_acc = 100. * correct_nano / N_TEST\n",
                "agreement = 100. * match_count / N_TEST\n",
                "total_flash = sum(info['weights'].nbytes for info in q_weights.values())\n",
                "\n",
                "print('=' * 60)\n",
                "print('       MNIST CLASSIFICATION RESULTS')\n",
                "print('=' * 60)\n",
                "print(f'PyTorch Accuracy:      {pt_accuracy:.2f}%')\n",
                "print(f'NANO-RUST Accuracy:    {nano_acc:.2f}% (n={N_TEST})')\n",
                "print(f'Classification Match:  {agreement:.1f}%')\n",
                "print(f'Max i8 Diff (median):  {int(np.median(max_diffs))}')\n",
                "print(f'Max i8 Diff (95th):    {int(np.percentile(max_diffs, 95))}')\n",
                "print(f'Flash: {total_flash:,} bytes ({total_flash/1024:.1f}KB) | Arena: 32KB')\n",
                "print('=' * 60)\n",
                "print(f'{\"‚úÖ PASS\" if agreement > 90 else \"‚ùå FAIL\"}: {agreement:.1f}% agreement')\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìù Key Takeaways\n",
                "\n",
                "- CNN on MNIST: ~97% float32 accuracy\n",
                "- i8 quantization + calibration preserves accuracy within ~2%\n",
                "- Total model: ~13KB Flash ‚Äî fits on virtually any MCU\n",
                "- Export to firmware via `export_to_rust()` for ESP32/STM32 deployment\n"
            ]
        }
    ]
}

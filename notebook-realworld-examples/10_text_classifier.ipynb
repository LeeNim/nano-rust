{
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üì∞ 10 ‚Äî Text Classification (Sentiment/Topic Analysis)\n",
                "\n",
                "**Bag-of-Words MLP for on-device text classification ‚Äî realistic for command parsing on ESP32**\n",
                "\n",
                "| Property | Value |\n",
                "|----------|-------|\n",
                "| **Task** | News topic classification |\n",
                "| **Categories** | World, Sports, Business, Sci/Tech |\n",
                "| **Input** | 200-dim bag-of-words vector |\n",
                "| **Architecture** | Dense(200‚Üí128) ‚Üí ReLU ‚Üí Dense(128‚Üí64) ‚Üí ReLU ‚Üí Dense(64‚Üí4) |\n",
                "| **MCU Memory** | ~34KB Flash + 8KB Arena |\n",
                "\n",
                "### Why Bag-of-Words on MCU?\n",
                "Full transformer models need MB of RAM. BoW + MLP needs <50KB total,\n",
                "making it viable for command parsing, alert classification, and simple NLP on edge devices.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import time\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import TensorDataset, DataLoader\n",
                "from collections import Counter\n",
                "from nano_rust_py.utils import quantize_to_i8, quantize_weights, calibrate_model\n",
                "import nano_rust_py\n",
                "\n",
                "CATEGORIES = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
                "VOCAB_SIZE = 200\n",
                "N_CLASSES = len(CATEGORIES)\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Device: {device}')\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Generate Text Data\n",
                "\n",
                "We use synthetic text with category-specific vocabulary distributions.\n",
                "Each category has 24 distinctive words + 29 shared common words.\n",
                "In production, replace with real AG News / custom dataset.\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "np.random.seed(42)\n",
                "\n",
                "WORD_POOLS = {\n",
                "    0: ['war','peace','president','election','government','minister','country','treaty',\n",
                "        'united','nations','policy','crisis','diplomacy','summit','conflict','border',\n",
                "        'refugee','military','security','alliance','vote','parliament','democracy','law'],\n",
                "    1: ['game','team','player','score','win','match','champion','league','season','goal',\n",
                "        'coach','tournament','final','record','olympic','medal','race','training',\n",
                "        'stadium','football','basketball','tennis','soccer','athlete'],\n",
                "    2: ['market','stock','price','company','profit','revenue','growth','economy','trade',\n",
                "        'investment','bank','finance','quarter','earnings','share','billion','dollar',\n",
                "        'ceo','merger','acquisition','startup','venture','inflation','tax'],\n",
                "    3: ['software','computer','data','internet','technology','research','science','algorithm',\n",
                "        'network','digital','system','device','robot','artificial','intelligence','quantum',\n",
                "        'chip','cloud','cyber','innovation','machine','learning','neural','genome'],\n",
                "}\n",
                "COMMON = ['the','is','was','are','been','have','had','will','said','new','year','first',\n",
                "          'also','would','could','after','more','about','between','has','their','from',\n",
                "          'other','been','made','world','time','just','most']\n",
                "\n",
                "def generate_text(n_per_class, n_words_range=(20, 50)):\n",
                "    texts, labels = [], []\n",
                "    for c in range(N_CLASSES):\n",
                "        pool = WORD_POOLS[c]\n",
                "        for _ in range(n_per_class):\n",
                "            n = np.random.randint(*n_words_range)\n",
                "            n_cat = int(n * 0.6)\n",
                "            words = list(np.random.choice(pool, n_cat, replace=True)) + \\\n",
                "                    list(np.random.choice(COMMON, n - n_cat, replace=True))\n",
                "            np.random.shuffle(words)\n",
                "            texts.append(' '.join(words))\n",
                "            labels.append(c)\n",
                "    return texts, labels\n",
                "\n",
                "train_texts, train_labels = generate_text(1000)\n",
                "test_texts, test_labels = generate_text(200)\n",
                "\n",
                "# Build vocabulary\n",
                "word_counts = Counter()\n",
                "for t in train_texts: word_counts.update(t.lower().split())\n",
                "vocab = [w for w, _ in word_counts.most_common(VOCAB_SIZE)]\n",
                "word2idx = {w: i for i, w in enumerate(vocab)}\n",
                "print(f'Vocab: {len(vocab)} words | Train: {len(train_texts)} | Test: {len(test_texts)}')\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def text_to_bow(text):\n",
                "    bow = np.zeros(VOCAB_SIZE, dtype=np.float32)\n",
                "    for w in text.lower().split():\n",
                "        if w in word2idx: bow[word2idx[w]] += 1\n",
                "    if bow.sum() > 0: bow /= bow.sum()\n",
                "    return bow\n",
                "\n",
                "X_train = np.array([text_to_bow(t) for t in train_texts])\n",
                "X_test = np.array([text_to_bow(t) for t in test_texts])\n",
                "y_train = np.array(train_labels, dtype=np.int64)\n",
                "y_test = np.array(test_labels, dtype=np.int64)\n",
                "idx = np.random.permutation(len(X_train))\n",
                "X_train, y_train = X_train[idx], y_train[idx]\n",
                "\n",
                "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
                "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, pin_memory=True)\n",
                "print('BoW features ready.')\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Train MLP (20 epochs)"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "model = nn.Sequential(\n",
                "    nn.Linear(VOCAB_SIZE, 128), nn.ReLU(),\n",
                "    nn.Linear(128, 64),         nn.ReLU(),\n",
                "    nn.Linear(64, N_CLASSES),\n",
                ").to(device)\n",
                "\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "EPOCHS = 20\n",
                "\n",
                "t0 = time.time()\n",
                "for epoch in range(EPOCHS):\n",
                "    model.train()\n",
                "    correct, total = 0, 0\n",
                "    for features, labels in train_loader:\n",
                "        features, labels = features.to(device), labels.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        out = model(features)\n",
                "        criterion(out, labels).backward()\n",
                "        optimizer.step()\n",
                "        correct += out.argmax(1).eq(labels).sum().item()\n",
                "        total += labels.size(0)\n",
                "    if (epoch + 1) % 5 == 0:\n",
                "        print(f'  Epoch {epoch+1}/{EPOCHS} ‚Äî Acc: {100.*correct/total:.1f}%')\n",
                "train_time = time.time() - t0\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Evaluate & Quantize"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "model.eval()\n",
                "correct_pt = 0\n",
                "with torch.no_grad():\n",
                "    pt_pred = model(torch.from_numpy(X_test).to(device)).argmax(1).cpu().numpy()\n",
                "correct_pt = (pt_pred == y_test).sum()\n",
                "pt_acc = 100. * correct_pt / len(y_test)\n",
                "print(f'PyTorch Test Accuracy: {pt_acc:.2f}%')\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "model_cpu = model.cpu().eval()\n",
                "q_weights = quantize_weights(model_cpu)\n",
                "\n",
                "global_max = float(np.max(np.abs(np.vstack([X_train, X_test]))))\n",
                "global_scale = global_max / 127.0\n",
                "print(f'Global scale: {global_scale:.6f}')\n",
                "\n",
                "def quantize_global(data):\n",
                "    return np.clip(np.round(data / global_scale), -128, 127).astype(np.int8)\n",
                "\n",
                "cal_input = torch.from_numpy(X_test[:1])\n",
                "requant = calibrate_model(model_cpu, cal_input, q_weights, global_scale)\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: NANO-RUST Test"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def build_nano():\n",
                "    nano = nano_rust_py.PySequentialModel(input_shape=[VOCAB_SIZE], arena_size=8192)\n",
                "    m, s, bc = requant['0']\n",
                "    nano.add_dense_with_requant(q_weights['0']['weights'].flatten().tolist(), bc, m, s)\n",
                "    nano.add_relu()\n",
                "    m, s, bc = requant['2']\n",
                "    nano.add_dense_with_requant(q_weights['2']['weights'].flatten().tolist(), bc, m, s)\n",
                "    nano.add_relu()\n",
                "    m, s, bc = requant['4']\n",
                "    nano.add_dense_with_requant(q_weights['4']['weights'].flatten().tolist(), bc, m, s)\n",
                "    return nano\n",
                "\n",
                "N_TEST = len(X_test)\n",
                "correct_nano, match_count, max_diffs = 0, 0, []\n",
                "t0 = time.time()\n",
                "for i in range(N_TEST):\n",
                "    q_feat = quantize_global(X_test[i])\n",
                "    label = int(y_test[i])\n",
                "    nano_out = build_nano().forward(q_feat.tolist())\n",
                "    nano_cls = int(np.argmax(nano_out))\n",
                "    with torch.no_grad():\n",
                "        pt_out = model_cpu(torch.from_numpy(X_test[i:i+1])).numpy().flatten()\n",
                "    pt_cls = int(np.argmax(pt_out))\n",
                "    q_pt, _ = quantize_to_i8(pt_out)\n",
                "    diff = np.abs(q_pt.astype(np.int32) - np.array(nano_out, dtype=np.int8).astype(np.int32))\n",
                "    max_diffs.append(int(np.max(diff)))\n",
                "    if nano_cls == label: correct_nano += 1\n",
                "    if nano_cls == pt_cls: match_count += 1\n",
                "infer_time = time.time() - t0\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Results"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "nano_acc = 100. * correct_nano / N_TEST\n",
                "agreement = 100. * match_count / N_TEST\n",
                "total_w = sum(q['weights'].nbytes for q in q_weights.values())\n",
                "print('=' * 60)\n",
                "print('       TEXT CLASSIFICATION RESULTS')\n",
                "print('=' * 60)\n",
                "print(f'Categories: {\", \".join(CATEGORIES)}')\n",
                "print(f'Vocab size: {VOCAB_SIZE}')\n",
                "print(f'PyTorch Accuracy:     {pt_acc:.2f}%')\n",
                "print(f'NANO-RUST Accuracy:   {nano_acc:.2f}% (n={N_TEST})')\n",
                "print(f'Classification Match: {agreement:.1f}%')\n",
                "print(f'Max Diff (median):    {int(np.median(max_diffs))}')\n",
                "print(f'Memory: {total_w:,} bytes ({total_w/1024:.1f}KB) + 8KB arena')\n",
                "print(f'Fits ESP32? {\"YES\" if total_w + 8192 < 520*1024 else \"NO\"}')\n",
                "print('=' * 60)\n",
                "print(f'{\"‚úÖ PASS\" if agreement > 85 else \"‚ùå FAIL\"}: {agreement:.1f}% agreement')\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìù Key Takeaways\n",
                "\n",
                "- Bag-of-Words + MLP is a viable NLP approach for MCUs\n",
                "- Total model size ~34KB ‚Äî fits comfortably on ESP32\n",
                "- Can be used for: command parsing, alert classification, spam filtering\n",
                "- For more complex NLP, consider DistilBERT ‚Üí knowledge distillation ‚Üí small MLP\n"
            ]
        }
    ]
}

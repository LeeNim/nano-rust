{
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üé§ 09 ‚Äî Keyword Spotting (Voice Command Detection)\n",
                "\n",
                "**Detect spoken keywords from audio MFCC features ‚Äî a core TinyML use case**\n",
                "\n",
                "| Property | Value |\n",
                "|----------|-------|\n",
                "| **Task** | Wake-word / command detection |\n",
                "| **Dataset** | Google Speech Commands v0.02 (~2.3GB) |\n",
                "| **Keywords** | yes, no, up, down, left, right, on, off, stop, go |\n",
                "| **Input** | 13 MFCC √ó 32 frames = 416 features |\n",
                "| **Architecture** | Dense(416‚Üí128) ‚Üí ReLU ‚Üí Dense(128‚Üí64) ‚Üí ReLU ‚Üí Dense(64‚Üí10) |\n",
                "| **MCU Memory** | ~62KB Flash + 16KB Arena |\n",
                "\n",
                "### Audio Processing Pipeline\n",
                "```\n",
                "WAV (16kHz) ‚Üí Pad/Truncate to 1s ‚Üí MFCC (13 coefficients √ó 32 frames)\n",
                "            ‚Üí Normalize ‚Üí Flatten [416] ‚Üí MLP ‚Üí [10 keywords]\n",
                "```\n",
                "\n",
                "> **Prerequisites**: `pip install nano-rust-py[train,audio]` (includes torchaudio, soundfile)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import time\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torchaudio\n",
                "import soundfile as sf\n",
                "from pathlib import Path\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "from nano_rust_py.utils import quantize_to_i8, quantize_weights, calibrate_model\n",
                "import nano_rust_py\n",
                "\n",
                "KEYWORDS = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
                "N_MFCC, N_FRAMES = 13, 32\n",
                "N_FEATURES = N_MFCC * N_FRAMES  # 416\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Device: {device}')\n",
                "print(f'Features: {N_MFCC} MFCC √ó {N_FRAMES} frames = {N_FEATURES}')\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Download & Load Speech Commands\n",
                "\n",
                "Downloads Google Speech Commands v0.02 (~2.3GB on first run).\n",
                "Audio is processed into MFCC features (13 coefficients √ó 32 time frames).\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import tarfile, urllib.request\n",
                "\n",
                "DATASET_URL = 'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
                "DATASET_DIR = Path('./data/SpeechCommands/speech_commands_v0.02')\n",
                "if not DATASET_DIR.exists():\n",
                "    DATASET_DIR = Path('./data/speech_commands_v002')\n",
                "\n",
                "if not DATASET_DIR.exists():\n",
                "    archive = Path('./data/speech_commands_v0.02.tar.gz')\n",
                "    archive.parent.mkdir(parents=True, exist_ok=True)\n",
                "    if not archive.exists():\n",
                "        print('Downloading Speech Commands dataset (~2.3GB)...')\n",
                "        urllib.request.urlretrieve(DATASET_URL, str(archive))\n",
                "    print('Extracting...')\n",
                "    DATASET_DIR.mkdir(parents=True, exist_ok=True)\n",
                "    with tarfile.open(str(archive), 'r:gz') as tar:\n",
                "        tar.extractall(str(DATASET_DIR))\n",
                "\n",
                "val_set = set((DATASET_DIR / 'validation_list.txt').read_text().strip().split('\\n'))\n",
                "test_set = set((DATASET_DIR / 'testing_list.txt').read_text().strip().split('\\n'))\n",
                "print(f'Dataset ready at: {DATASET_DIR}')\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "class KeywordDataset(Dataset):\n",
                "    def __init__(self, subset='training'):\n",
                "        self.mfcc_transform = torchaudio.transforms.MFCC(\n",
                "            sample_rate=16000, n_mfcc=N_MFCC,\n",
                "            melkwargs={'n_fft': 400, 'hop_length': 160, 'n_mels': 23}\n",
                "        )\n",
                "        self.files, self.labels = [], []\n",
                "        for kw in KEYWORDS:\n",
                "            kw_dir = DATASET_DIR / kw\n",
                "            if not kw_dir.exists(): continue\n",
                "            for wav in kw_dir.glob('*.wav'):\n",
                "                rel = f'{kw}/{wav.name}'\n",
                "                if subset == 'testing' and rel not in test_set: continue\n",
                "                elif subset == 'validation' and rel not in val_set: continue\n",
                "                elif subset == 'training' and (rel in test_set or rel in val_set): continue\n",
                "                self.files.append(str(wav))\n",
                "                self.labels.append(KEYWORDS.index(kw))\n",
                "        print(f'  {subset}: {len(self.files)} samples')\n",
                "\n",
                "    def __len__(self): return len(self.files)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        audio_np, _ = sf.read(self.files[idx], dtype='float32')\n",
                "        waveform = torch.from_numpy(audio_np).unsqueeze(0)\n",
                "        if waveform.shape[1] < 16000:\n",
                "            waveform = torch.nn.functional.pad(waveform, (0, 16000 - waveform.shape[1]))\n",
                "        else:\n",
                "            waveform = waveform[:, :16000]\n",
                "        mfcc = self.mfcc_transform(waveform).squeeze(0)\n",
                "        if mfcc.shape[1] != N_FRAMES:\n",
                "            mfcc = torch.nn.functional.interpolate(\n",
                "                mfcc.unsqueeze(0), size=N_FRAMES, mode='linear', align_corners=False\n",
                "            ).squeeze(0)\n",
                "        features = mfcc.flatten()\n",
                "        features = (features - features.mean()) / (features.std() + 1e-8)\n",
                "        return features, self.labels[idx]\n",
                "\n",
                "train_ds = KeywordDataset('training')\n",
                "test_ds = KeywordDataset('testing')\n",
                "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True, pin_memory=True, num_workers=0)\n",
                "test_loader = DataLoader(test_ds, batch_size=256, shuffle=False, pin_memory=True, num_workers=0)\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Train MLP (10 epochs)"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "model = nn.Sequential(\n",
                "    nn.Linear(N_FEATURES, 128), nn.ReLU(),\n",
                "    nn.Linear(128, 64),         nn.ReLU(),\n",
                "    nn.Linear(64, len(KEYWORDS)),\n",
                ").to(device)\n",
                "\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "EPOCHS = 10\n",
                "\n",
                "t0 = time.time()\n",
                "for epoch in range(EPOCHS):\n",
                "    model.train()\n",
                "    correct, total = 0, 0\n",
                "    for features, labels in train_loader:\n",
                "        features, labels = features.to(device), labels.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        out = model(features)\n",
                "        criterion(out, labels).backward()\n",
                "        optimizer.step()\n",
                "        correct += out.argmax(1).eq(labels).sum().item()\n",
                "        total += labels.size(0)\n",
                "    if (epoch + 1) % 2 == 0:\n",
                "        print(f'  Epoch {epoch+1}/{EPOCHS} ‚Äî Acc: {100.*correct/total:.1f}%')\n",
                "train_time = time.time() - t0\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Evaluate & Quantize"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "model.eval()\n",
                "correct_pt, total_pt = 0, 0\n",
                "with torch.no_grad():\n",
                "    for features, labels in test_loader:\n",
                "        features, labels = features.to(device), labels.to(device)\n",
                "        correct_pt += model(features).argmax(1).eq(labels).sum().item()\n",
                "        total_pt += labels.size(0)\n",
                "pt_acc = 100. * correct_pt / total_pt\n",
                "print(f'PyTorch Test Accuracy: {pt_acc:.2f}%')\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "model_cpu = model.cpu().eval()\n",
                "q_weights = quantize_weights(model_cpu)\n",
                "\n",
                "# Global scale from test features\n",
                "all_feats = np.vstack([test_ds[i][0].numpy() for i in range(min(100, len(test_ds)))])\n",
                "global_max = float(np.max(np.abs(all_feats)))\n",
                "global_scale = global_max / 127.0\n",
                "print(f'Global scale: {global_scale:.6f}')\n",
                "\n",
                "def quantize_global(data):\n",
                "    return np.clip(np.round(data / global_scale), -128, 127).astype(np.int8)\n",
                "\n",
                "cal_input = test_ds[0][0].unsqueeze(0)\n",
                "requant = calibrate_model(model_cpu, cal_input, q_weights, global_scale)\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: NANO-RUST Test (500 samples)"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def build_nano():\n",
                "    nano = nano_rust_py.PySequentialModel(input_shape=[N_FEATURES], arena_size=16384)\n",
                "    m, s, bc = requant['0']\n",
                "    nano.add_dense_with_requant(q_weights['0']['weights'].flatten().tolist(), bc, m, s)\n",
                "    nano.add_relu()\n",
                "    m, s, bc = requant['2']\n",
                "    nano.add_dense_with_requant(q_weights['2']['weights'].flatten().tolist(), bc, m, s)\n",
                "    nano.add_relu()\n",
                "    m, s, bc = requant['4']\n",
                "    nano.add_dense_with_requant(q_weights['4']['weights'].flatten().tolist(), bc, m, s)\n",
                "    return nano\n",
                "\n",
                "N_TEST = min(500, len(test_ds))\n",
                "correct_nano, match_count, max_diffs = 0, 0, []\n",
                "t0 = time.time()\n",
                "for i in range(N_TEST):\n",
                "    feat, label = test_ds[i]\n",
                "    q_feat = quantize_global(feat.numpy())\n",
                "    nano_out = build_nano().forward(q_feat.tolist())\n",
                "    nano_cls = int(np.argmax(nano_out))\n",
                "    with torch.no_grad():\n",
                "        pt_out = model_cpu(feat.unsqueeze(0)).numpy().flatten()\n",
                "    pt_cls = int(np.argmax(pt_out))\n",
                "    q_pt, _ = quantize_to_i8(pt_out)\n",
                "    diff = np.abs(q_pt.astype(np.int32) - np.array(nano_out, dtype=np.int8).astype(np.int32))\n",
                "    max_diffs.append(int(np.max(diff)))\n",
                "    if nano_cls == label: correct_nano += 1\n",
                "    if nano_cls == pt_cls: match_count += 1\n",
                "    if (i+1) % 100 == 0: print(f'  {i+1}/{N_TEST}...')\n",
                "infer_time = time.time() - t0\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Results"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "nano_acc = 100. * correct_nano / N_TEST\n",
                "agreement = 100. * match_count / N_TEST\n",
                "total_w = sum(q['weights'].nbytes for q in q_weights.values())\n",
                "print('=' * 60)\n",
                "print('       KEYWORD SPOTTING RESULTS')\n",
                "print('=' * 60)\n",
                "print(f'Keywords: {\", \".join(KEYWORDS)}')\n",
                "print(f'PyTorch Accuracy:     {pt_acc:.2f}%')\n",
                "print(f'NANO-RUST Accuracy:   {nano_acc:.2f}% (n={N_TEST})')\n",
                "print(f'Classification Match: {agreement:.1f}%')\n",
                "print(f'Max Diff (median):    {int(np.median(max_diffs))}')\n",
                "print(f'Memory: {total_w:,} bytes ({total_w/1024:.1f}KB) + 16KB arena')\n",
                "print(f'Fits ESP32 (520KB)? {\"YES\" if total_w + 16384 < 520*1024 else \"NO\"}')\n",
                "print('=' * 60)\n",
                "print(f'{\"‚úÖ PASS\" if agreement > 85 else \"‚ùå FAIL\"}: {agreement:.1f}% agreement')\n"
            ],
            "outputs": [],
            "execution_count": null
        }
    ]
}
